# Phase 4 å®Œæ•´å¯¦ä½œæŒ‡å—

> **ç‹€æ…‹**: âœ… å·²å®Œæˆ (2025-12-10)
> **æ¸¬è©¦**: 104 passed

---

## Phase 4.1: LLM Factory é›™æ¨¡å‹æ”¯æ´

### ğŸ“ ä¿®æ”¹æª”æ¡ˆ: `core/llm_factory.py`

#### è®Šæ›´å…§å®¹

```python
# æ–°å¢æ¨¡å‹æ˜ å°„ (Line ~25)
_MODEL_BY_PURPOSE: dict[str, str] = {
    "translation": "gemini-3.0-flash",
}
_DEFAULT_MODEL = "gemma-3-27b-it"

# ä¿®æ”¹ get_llm() å‡½å¼
def get_llm(purpose: LLMPurpose) -> ChatGoogleGenerativeAI:
    model = _MODEL_BY_PURPOSE.get(purpose, _DEFAULT_MODEL)
    config = _PURPOSE_CONFIG.get(purpose, _PURPOSE_CONFIG["rag_qa"])
    return ChatGoogleGenerativeAI(model=model, **config)
```

#### æ¨¡å‹åˆ†é…

| ç”¨é€”          | æ¨¡å‹               | åŸå›                     |
| ------------- | ------------------ | ----------------------- |
| `translation` | `gemini-3.0-flash` | é«˜è¼¸å‡ºé™åˆ¶ (65K tokens) |
| å…¶ä»–æ‰€æœ‰      | `gemma-3-27b-it`   | æ¨ç†å“è³ªè¼ƒå¥½            |

---

## Phase 4.2: ç¿»è­¯é é¢åˆ†å¡Š

### ğŸ“ æ–°å¢æª”æ¡ˆ: `pdfserviceMD/translation_chunker.py`

#### å®Œæ•´ç¨‹å¼ç¢¼çµæ§‹

```python
"""
Translation Chunker Module

Provides page-based chunking for large document translation.
Splits markdown by [[PAGE_N]] markers and batches pages based on output token limits.
"""

import logging
import re
from typing import List, Tuple

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from core.llm_factory import get_llm

logger = logging.getLogger(__name__)

MAX_OUTPUT_TOKENS = 60000  # Buffer below 65K limit
CHARS_PER_TOKEN_ESTIMATE = 1.33


def estimate_tokens(text: str) -> int:
    """Estimates tokens (~0.75 tokens per char for Chinese)."""
    return int(len(text) / CHARS_PER_TOKEN_ESTIMATE)


def split_by_page_markers(markdown: str) -> List[Tuple[int, str]]:
    """Splits markdown by [[PAGE_N]] markers."""
    pattern = r"\[\[PAGE_(\d+)\]\]"
    parts = re.split(pattern, markdown)

    pages: List[Tuple[int, str]] = []
    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            page_num = int(parts[i])
            content = parts[i + 1].strip()
            if content:
                pages.append((page_num, content))
    return pages


def batch_pages(
    pages: List[Tuple[int, str]],
    max_output_tokens: int = MAX_OUTPUT_TOKENS
) -> List[List[Tuple[int, str]]]:
    """Batches pages respecting output token limits."""
    batches: List[List[Tuple[int, str]]] = []
    current_batch: List[Tuple[int, str]] = []
    current_tokens = 0

    for page_num, content in pages:
        estimated_output = estimate_tokens(content) * 1.2

        if current_tokens + estimated_output > max_output_tokens and current_batch:
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0

        current_batch.append((page_num, content))
        current_tokens += estimated_output

    if current_batch:
        batches.append(current_batch)

    return batches


async def translate_single_page(content: str) -> str:
    """Translates a single page without markers."""
    template = """ä½ æ˜¯ä¸€å€‹ç¿»è­¯åŠ©æ‰‹ã€‚è«‹å°‡ä»¥ä¸‹ Markdown æ–‡å­—ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

    æ³¨æ„ï¼š
    1. ä¿ç•™æ‰€æœ‰ Markdown çµæ§‹
    2. ä¿ç•™æ‰€æœ‰ [IMG_PLACEHOLDER_X] æ¨™è¨˜
    3. åƒ…ç¿»è­¯è‹±æ–‡æ–‡å­—
    4. ç›´æ¥è¼¸å‡º Markdownï¼Œä¸è¦åŠ è¨»èªªæ˜

    Markdown å…§å®¹:
    {input_text}
    """

    prompt = ChatPromptTemplate.from_template(template)
    llm = get_llm("translation")
    chain = prompt | llm | StrOutputParser()

    try:
        return await chain.ainvoke({"input_text": content})
    except Exception as e:
        logger.error(f"Page translation failed: {e}")
        return content  # Graceful degradation


async def translate_batch(batch: List[Tuple[int, str]]) -> str:
    """Translates batch, adding markers ourselves."""
    translated_pages: List[str] = []

    for page_num, content in batch:
        translated = await translate_single_page(content)
        # WE add marker back - not relying on LLM
        translated_pages.append(f"[[PAGE_{page_num}]]\n{translated}")

    return "\n\n".join(translated_pages)


async def translate_chunked(markdown: str) -> str:
    """Main entry point for chunked translation."""
    pages = split_by_page_markers(markdown)

    if not pages:
        return await translate_batch([(1, markdown)])

    batches = batch_pages(pages)

    if len(batches) == 1:
        return await translate_batch(batches[0])

    translated_batches = []
    for batch in batches:
        translated = await translate_batch(batch)
        translated_batches.append(translated)

    return "\n\n".join(translated_batches)
```

### ğŸ“ ä¿®æ”¹æª”æ¡ˆ: `pdfserviceMD/ai_translate_md.py`

```python
from pdfserviceMD.translation_chunker import translate_chunked

async def translate_text(text: str) -> str:
    """Translates text using page-based chunking."""
    if not text or not text.strip():
        return text

    try:
        result = await translate_chunked(text)
        return result
    except Exception as e:
        logger.error(f"Translation failed: {e}")
        return text
```

---

## Phase 4.3: äº¤éŒ¯å¼å¤šæ¨¡æ…‹å•ç­”

### ğŸ“ ä¿®æ”¹æª”æ¡ˆ: `data_base/RAG_QA_service.py`

#### æ–°æç¤ºçµæ§‹ (Line ~221)

```python
# Step 8: Build interleaved multimodal message
context_text = "\n\n---\n\n".join(text_context) if text_context else "(ç„¡æ–‡å­—èƒŒæ™¯è³‡è¨Š)"

prompt_text = f"""ä½ æ˜¯ä¸€ä½å­¸è¡“ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•·åˆ†ææ–‡æœ¬èˆ‡åœ–è¡¨ã€‚

## åƒè€ƒè³‡æ–™
ä»¥ä¸‹æ˜¯å¾çŸ¥è­˜åº«æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹ï¼š

{context_text}

## ä½¿ç”¨è€…å•é¡Œ
{question}

## å›ç­”æŒ‡å¼•
1. ä»”ç´°è§€å¯Ÿåœ–è¡¨/åœ–ç‰‡ä¸­çš„æ•¸æ“šèˆ‡è¶¨å‹¢ï¼ˆå¦‚æœ‰æä¾›ï¼‰
2. çµåˆæ–‡å­—å…§å®¹èˆ‡åœ–ç‰‡è³‡è¨Šé€²è¡Œæ¨ç†
3. å¼•ç”¨å…·é«”ä¾†æºæ™‚ï¼Œèªªæ˜è³‡è¨Šå‡ºè™•
4. æ•¸å­¸å…¬å¼è«‹ä½¿ç”¨ LaTeX æ ¼å¼ (ä¾‹å¦‚ $\\frac{{a}}{{b}}$)
5. ä»¥ç¹é«”ä¸­æ–‡å›ç­”
6. å¦‚æœåœ–ç‰‡èˆ‡å•é¡Œç„¡é—œï¼Œè«‹å¿½ç•¥åœ–ç‰‡

è«‹æ ¹æ“šä»¥ä¸Šè³‡æ–™å›ç­”å•é¡Œï¼š"""
```

---

## æ¸¬è©¦é©—è­‰

### è‡ªå‹•æ¸¬è©¦

```powershell
cd d:\flutterserver\pdftopng
pytest tests/ -v
# çµæœ: 104 passed
```

### æ‰‹å‹•æ¸¬è©¦æŒ‡ä»¤

```powershell
# 1. å•Ÿå‹•æœå‹™å™¨
uvicorn main:app --reload --port 8000

# 2. ä¸Šå‚³ PDF + OCR + ç¿»è­¯
curl.exe -X POST "http://localhost:8000/pdfmd/ocr" `
  -F "file=@nnunetv2.pdf" -o translated.pdf

# 3. å•ç­”æ¸¬è©¦
curl.exe -G "http://localhost:8000/rag/ask" `
  --data-urlencode "question=ä»€éº¼æ˜¯nnU-Net"

# 4. æ·±åº¦ç ”ç©¶æ¸¬è©¦
python -c "import httpx; r=httpx.post('http://localhost:8000/rag/research', json={'question':'æ¯”è¼ƒnnU-Netèˆ‡U-Netçš„å·®ç•°'}, timeout=120); print(r.json()['summary'])"
```

---

## ä¿®å¾©ç´€éŒ„

### Bug: ç¿»è­¯å¾Œ PDF åªæœ‰ 3 é 

**åŸå› **: LLM ç¿»è­¯æ™‚åˆªé™¤ `[[PAGE_N]]` æ¨™è¨˜

**ä¿®å¾©**: æ”¹ç‚ºé€é ç¿»è­¯ï¼Œæˆ‘å€‘è‡ªå·±æ·»åŠ  `[[PAGE_N]]` æ¨™è¨˜

```python
# Before (å•é¡Œ)
translated = await llm.invoke(combined_with_markers)

# After (ä¿®å¾©)
for page_num, content in batch:
    translated = await translate_single_page(content)
    result.append(f"[[PAGE_{page_num}]]\n{translated}")  # æˆ‘å€‘åŠ å›æ¨™è¨˜
```

---

## ç›¸é—œæª”æ¡ˆæ¸…å–®

| æª”æ¡ˆ                                  |   æ“ä½œ   | èªªæ˜         |
| ------------------------------------- | :------: | ------------ |
| `core/llm_factory.py`                 |   ä¿®æ”¹   | é›™æ¨¡å‹æ”¯æ´   |
| `pdfserviceMD/translation_chunker.py` | **æ–°å¢** | é é¢åˆ†å¡Šç¿»è­¯ |
| `pdfserviceMD/ai_translate_md.py`     |   ä¿®æ”¹   | ä½¿ç”¨åˆ†å¡Šç¿»è­¯ |
| `data_base/RAG_QA_service.py`         |   ä¿®æ”¹   | äº¤éŒ¯å¼æç¤º   |
